
@ARTICLE{Harvey2017,
  title     = "Presidential address: The scientific outlook in financial
               economics: Scientific outlook in finance",
  author    = "Harvey, Campbell R",
  abstract  = "ABSTRACT Given the competition for top journal space, there is
               an incentive to produce ?significant? results. With the
               combination of unreported tests, lack of adjustment for multiple
               tests, and direct and indirect p-hacking, many of the results
               being published will fail to hold up in the future. In addition,
               there are basic issues with the interpretation of statistical
               significance. Increasing thresholds may be necessary, but still
               may not be sufficient: if the effect being studied is rare, even
               t > 3 will produce a large number of false positives. Here I
               explore the meaning and limitations of a p-value. I offer a
               simple alternative (the minimum Bayes factor). I present
               guidelines for a robust, transparent research culture in
               financial economics. Finally, I offer some thoughts on the
               importance of risk-taking (from the perspective of authors and
               editors) to advance our field. SUMMARY Empirical research in
               financial economics relies too much on p-values, which are
               poorly understood in the first place. Journals want to publish
               papers with positive results and this incentivizes researchers
               to engage in data mining and ?p-hacking.? The outcome will
               likely be an embarrassing number of false positives?effects that
               will not be repeated in the future. The minimum Bayes factor
               (which is a function of the p-value) combined with prior odds
               provides a simple solution that can be reported alongside the
               usual p-value. The Bayesianized p-value answers the question:
               What is the probability that the null is true? The same
               technique can be used to answer: What threshold of t-statistic
               do I need so that there is only a 5\% chance that the null is
               true? The threshold depends on the economic plausibility of the
               hypothesis.",
  journal   = "J. Finance",
  publisher = "Wiley",
  volume    =  72,
  number    =  4,
  pages     = "1399--1440",
  month     =  aug,
  year      =  2017,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}


@ARTICLE{Blei2016,
  title         = "Variational Inference: A Review for Statisticians",
  author        = "Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D",
  abstract      = "One of the core problems of modern statistics is to
                   approximate difficult-to-compute probability densities. This
                   problem is especially important in Bayesian statistics,
                   which frames all inference about unknown quantities as a
                   calculation involving the posterior density. In this paper,
                   we review variational inference (VI), a method from machine
                   learning that approximates probability densities through
                   optimization. VI has been used in many applications and
                   tends to be faster than classical methods, such as Markov
                   chain Monte Carlo sampling. The idea behind VI is to first
                   posit a family of densities and then to find the member of
                   that family which is close to the target. Closeness is
                   measured by Kullback-Leibler divergence. We review the ideas
                   behind mean-field variational inference, discuss the special
                   case of VI applied to exponential family models, present a
                   full example with a Bayesian mixture of Gaussians, and
                   derive a variant that uses stochastic optimization to scale
                   up to massive data. We discuss modern research in VI and
                   highlight important open problems. VI is powerful, but it is
                   not yet well understood. Our hope in writing this paper is
                   to catalyze statistical research on this class of
                   algorithms.",
  month         =  jan,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "1601.00670"
}


@BOOK{Lopez2018,
  title     = "Advances in Financial Machine Learning",
  author    = "L{\'o}pez de Prado, Marcos",
  abstract  = "Machine learning (ML) is changing virtually every aspect of our
               lives. Today ML algorithms accomplish tasks that until recently
               only expert humans could perform. As it relates to finance, this
               is the most exciting time to adopt a disruptive technology that
               will transform how everyone invests for generations. Readers
               will learn how to structure Big data in a way that is amenable
               to ML algorithms; how to conduct research with ML algorithms on
               that data; how to use supercomputing methods; how to backtest
               your discoveries while avoiding false positives. The book
               addresses real-life problems faced by practitioners on a daily
               basis, and explains scientifically sound solutions using math,
               supported by code and examples. Readers become active users who
               can test the proposed solutions in their particular setting.
               Written by a recognized expert and portfolio manager, this book
               will equip investment professionals with the groundbreaking
               tools needed to succeed in modern finance.",
  publisher = "John Wiley \& Sons",
  month     =  feb,
  year      =  2018,
  language  = "en"
}


@ARTICLE{Raymer2007,
  title    = "Spontaneous knotting of an agitated string",
  author   = "Raymer, Dorian M and Smith, Douglas E",
  abstract = "It is well known that a jostled string tends to become knotted;
              yet the factors governing the ``spontaneous'' formation of
              various knots are unclear. We performed experiments in which a
              string was tumbled inside a box and found that complex knots
              often form within seconds. We used mathematical knot theory to
              analyze the knots. Above a critical string length, the
              probability P of knotting at first increased sharply with length
              but then saturated below 100\%. This behavior differs from that
              of mathematical self-avoiding random walks, where P has been
              proven to approach 100\%. Finite agitation time and jamming of
              the string due to its stiffness result in lower probability, but
              P approaches 100\% with long, flexible strings. We analyzed the
              knots by calculating their Jones polynomials via computer
              analysis of digital photos of the string. Remarkably, almost all
              were identified as prime knots: 120 different types, having
              minimum crossing numbers up to 11, were observed in 3,415 trials.
              All prime knots with up to seven crossings were observed. The
              relative probability of forming a knot decreased exponentially
              with minimum crossing number and M{\"o}bius energy, mathematical
              measures of knot complexity. Based on the observation that long,
              stiff strings tend to form a coiled structure when confined, we
              propose a simple model to describe the knot formation based on
              random ``braid moves'' of the string end. Our model can
              qualitatively account for the observed distribution of knots and
              dependence on agitation time and string length.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  104,
  number   =  42,
  pages    = "16432--16437",
  month    =  oct,
  year     =  2007,
  language = "en"
}

@INCOLLECTION{Lopez2020,
  title     = "Machine Learning for Asset Managers",
  booktitle = "Elements in Quantitative Finance",
  author    = "L{\'o}pez de Prado, Marcos",
  abstract  = "Cambridge Core - Finance and Accountancy - Machine Learning for
               Asset Managers",
  publisher = "Cambridge University Press",
  month     =  apr,
  year      =  2020
}
 @Article{RTE2019,
    title = {RTransferEntropy â€” Quantifying information flow between different time series using effective transfer entropy},
    author = {Behrendt Simon and Dimpfl Thomas and Peter {Franziska J.} and Zimmermann {David J.}},
    journal = {SoftwareX},
    year = {2019},
    volume = {10},
    number = {100265},
    pages = {1-9},
    url = {https://doi.org/10.1016/j.softx.2019.100265},
  }

@INPROCEEDINGS{Ding2004,
  title     = "K-means clustering via principal component analysis",
  booktitle = "Proceedings of the twenty-first international conference on
               Machine learning",
  author    = "Ding, Chris and He, Xiaofeng",
  abstract  = "Principal component analysis (PCA) is a widely used statistical
               technique for unsupervised dimension reduction. K-means
               clustering is a commonly used data clustering for performing
               unsupervised learning tasks. Here we prove that principal
               components are the continuous solutions to the discrete cluster
               membership indicators for K-means clustering. New lower bounds
               for K-means objective function are derived, which is the total
               variance minus the eigenvalues of the data covariance matrix.
               These results indicate that unsupervised dimension reduction is
               closely related to unsupervised learning. Several implications
               are discussed. On dimension reduction, the result provides new
               insights to the observed effectiveness of PCA-based data
               reductions, beyond the conventional noise-reduction explanation
               that PCA, via singular value decomposition, provides the best
               low-dimensional linear approximation of the data. On learning,
               the result suggests effective techniques for K-means data
               clustering. DNA gene expression and Internet newsgroups are
               analyzed to illustrate our results. Experiments indicate that
               the new bounds are within 0.5-1.5\% of the optimal values.",
  publisher = "Association for Computing Machinery",
  pages     = "29",
  series    = "ICML '04",
  month     =  jul,
  year      =  2004,
  address   = "New York, NY, USA",
  location  = "Banff, Alberta, Canada"
}

@INCOLLECTION{Steinbach2004,
  title     = "The Challenges of Clustering High Dimensional Data",
  booktitle = "New Directions in Statistical Physics: Econophysics,
               Bioinformatics, and Pattern Recognition",
  author    = "Steinbach, Michael and Ert{\"o}z, Levent and Kumar, Vipin",
  editor    = "Wille, Luc T",
  abstract  = "Cluster analysis divides data into groups (clusters) for the
               purposes of summarization or improved understanding. For
               example, cluster analysis has been used to group related
               documents for browsing, to find genes and proteins that have
               similar functionality, or as a means of data compression. While
               clustering has a long history and a large number of clustering
               techniques have been developed in statistics, pattern
               recognition, data mining, and other fields, significant
               challenges still remain. In this chapter we provide a short
               introduction to cluster analysis, and then focus on the
               challenge of clustering high dimensional data. We present a
               brief overview of several recent techniques, including a more
               detailed description of recent work of our own which uses a
               concept-based clustering approach.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "273--309",
  year      =  2004,
  address   = "Berlin, Heidelberg"
}


@UNPUBLISHED{Prado2018,
  title    = "Detection of False Investment Strategies Using Unsupervised
              Learning Methods",
  author   = "Lopez de Prado, Marcos and Lewis, Michael J",
  abstract = "Most investment strategies uncovered by practitioners and
              academics are false. This partially explains the high rate of
              failure, especially among quantitative hedge funds (smart beta,
              factor investing, stat-arb, CTAs, etc.) In this paper we examine
              why false positives are so prevalent in finance, why researchers
              fail (in many cases purposely) to detect them, and why firms are
              able to monetize their scheme. Beyond merely pointing to this
              industrywide problem, we offer a practical solution. We hope that
              the machine learning tools presented in this paper will help
              financial academic journals filter out false positives, and bring
              up the retraction rate to reasonable levels. The SEC, FINRA and
              other regulatory agencies worldwide could use these tools to take
              a more active role in curving this rampant financial fraud. A
              presentation based on this paper can be found at
              https://ssrn.com/abstract=3173146.",
  month    =  aug,
  year     =  2018,
  keywords = "Backtest overfitting, selection bias, multiple testing,
              quantitative investments, machine learning, financial fraud"
}

@ARTICLE{Prado2016,
  title   = "Building Diversified Portfolios that Outperform {Out-of-Sample}",
  author  = "Lopez de Prado, Marcos",
  journal = "Journal of Portfolio Management",
  volume  =  42,
  number  =  4,
  pages   = "59--69",
  year    =  2016
}


@ARTICLE{Goutte1999,
  title    = "On clustering {fMRI} time series",
  author   = "Goutte, C and Toft, P and Rostrup, E and Nielsen, F and Hansen, L
              K",
  abstract = "Analysis of fMRI time series is often performed by extracting one
              or more parameters for the individual voxels. Methods based,
              e.g., on various statistical tests are then used to yield
              parameters corresponding to probability of activation or
              activation strength. However, these methods do not indicate
              whether sets of voxels are activated in a similar way or in
              different ways. Typically, delays between two activated signals
              are not identified. In this article, we use clustering methods to
              detect similarities in activation between voxels. We employ a
              novel metric that measures the similarity between the activation
              stimulus and the fMRI signal. We present two different clustering
              algorithms and use them to identify regions of similar
              activations in an fMRI experiment involving a visual stimulus.",
  journal  = "Neuroimage",
  volume   =  9,
  number   =  3,
  pages    = "298--310",
  month    =  mar,
  year     =  1999,
  language = "en"
}


@ARTICLE{Tibshirani2001,
  title     = "Estimating the number of clusters in a data set via the gap
               statistic",
  author    = "Tibshirani, Robert and Walther, Guenther and Hastie, Trevor",
  abstract  = "We propose a method (the ?gap statistic?) for estimating the
               number of clusters (groups) in a set of data. The technique uses
               the output of any clustering algorithm (e.g. K-means or
               hierarchical), comparing the change in within-cluster dispersion
               with that expected under an appropriate reference null
               distribution. Some theory is developed for the proposal and a
               simulation study shows that the gap statistic usually
               outperforms other methods that have been proposed in the
               literature.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley",
  volume    =  63,
  number    =  2,
  pages     = "411--423",
  year      =  2001,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}


